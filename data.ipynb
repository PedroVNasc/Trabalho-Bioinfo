{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import json\n",
    "import dill\n",
    "import wget\n",
    "from scipy.fftpack import fft\n",
    "from Bio import Entrez\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente é feita a filtragem dos Dados iniciais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./data/raw', exist_ok=True)\n",
    "\n",
    "classes = ['LTR', 'LINE', 'SINE', 'TIR', 'MITE', 'Helitron']\n",
    "\n",
    "file_paths = os.listdir('./data/raw')\n",
    "\n",
    "for name in classes:\n",
    "    if f'TEAnnotationFinal_{name}.gff3' not in file_paths:\n",
    "        wget.download(f'http://apte.cp.utfpr.edu.br/te-annotation/zea_mays/TEAnnotationFinal_{name}.gff3',\n",
    "                      out=f'./data/raw/')\n",
    "\n",
    "file_paths = os.listdir('./data/raw')\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame()\n",
    "\n",
    "if not os.path.isfile('./data/classes.csv'):\n",
    "    data = []\n",
    "\n",
    "    for path in file_paths:\n",
    "        with open(f'./data/raw/{path}', 'r') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "\n",
    "            class_name = path.split('.')[0].split('_')[1]\n",
    "\n",
    "            for row in reader:\n",
    "                if (row[6] == '+'):\n",
    "                    data.append([class_name] + row)\n",
    "\n",
    "    header = ['Class', 'Chr', 'Source Annotation', 'Class/Order/Superfamily', 'Start', 'End', 'Score', 'Strand', 'Phase', 'Attributes']\n",
    "\n",
    "    data_df = pd.DataFrame(data, columns=header)\n",
    "    data_df = data_df[['Class', 'Chr', 'Start', 'End']]\n",
    "\n",
    "    data_df.to_csv('./data/classes.csv')\n",
    "\n",
    "else:\n",
    "    data_df = pd.read_csv('./data/classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Start'] = data_df['Start'].astype('int64')\n",
    "data_df['End'] = data_df['End'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em sequência será feita a extração dos genomas por meio de Biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dict = {\"LR618874.1\": \"Chr_1.txt\", \"LR618875.1\": \"Chr_2.txt\", \"LR618876.1\": \"Chr_3.txt\", \"LR618877.1\": \"Chr_4.txt\", \n",
    "           \"LR618878.1\": \"Chr_5.txt\", \"LR618879.1\": \"Chr_6.txt\", \"LR618880.1\": \"Chr_7.txt\", \"LR618881.1\": \"Chr_8.txt\", \n",
    "           \"LR618882.1\": \"Chr_9.txt\", \"LR618883.1\": \"Chr_10.txt\", \"AY506529.1\":\"Chr_Mt.txt\", \"X86563.2\": \"Chr_Pt.txt\"}\n",
    "\n",
    "Entrez.email = \"pedro.guilherme2305@usp.br\"\n",
    "\n",
    "os.makedirs('./data/sequences', exist_ok=True)\n",
    "\n",
    "for id in tqdm(id_dict, total=len(id_dict)):\n",
    "    if not os.path.isfile(f'./data/sequences/{id_dict[id]}'):\n",
    "        stream = Entrez.efetch(db=\"nuccore\", id=id, rettype=\"fasta\")\n",
    "\n",
    "        with open(f\"./data/sequences/{id_dict[id]}\", \"w\") as file:\n",
    "            file.write(stream.read())\n",
    "        stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(data_df: pd.DataFrame):\n",
    "    chromosomes_to_keep = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'Mt', 'Pt']\n",
    "    data_df = data_df.query(\"Chr in @chromosomes_to_keep\") \n",
    "\n",
    "    aux_list = []\n",
    "\n",
    "    for chromosome in chromosomes_to_keep:\n",
    "        rows = data_df.query(f\"Chr == '{chromosome}'\").to_dict(orient=\"records\")\n",
    "        record = SeqIO.read(f\"./data/sequences/Chr_{chromosome}.txt\", \"fasta\")\n",
    "        for row in tqdm(rows, total=len(rows)):\n",
    "            aux_dict = dict()\n",
    "\n",
    "            aux_dict['Chr'] = row['Chr']\n",
    "            aux_dict['Sequence'] = record[row['Start']:row['End']].seq\n",
    "            aux_dict['Class'] = row['Class']\n",
    "\n",
    "            if aux_dict['Sequence'] == '': aux_dict['Sequence'] = np.nan\n",
    "\n",
    "            aux_list.append(aux_dict)\n",
    "    \n",
    "    return aux_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feauture Extraction - Accumulated Nucle Frequency Fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(spectrum, spectrumTwo):\n",
    "    features = []\n",
    "\n",
    "    average = sum(spectrum)/len(spectrum)\n",
    "    features.append(average)\n",
    "    ###################################\n",
    "    median = np.median(spectrum)\n",
    "    features.append(median)\n",
    "\t###################################\n",
    "    maximum = np.max(spectrum)\n",
    "    features.append(maximum)\n",
    "    ###################################\n",
    "    minimum = np.min(spectrum)\n",
    "    features.append(minimum)\n",
    "    ###################################\n",
    "    peak = (len(spectrum)/3)/(average)\n",
    "    features.append(peak)\n",
    "    ###################################\n",
    "    peak_two = (len(spectrumTwo)/3)/(np.mean(spectrumTwo))\n",
    "    features.append(peak_two)\n",
    "    ###################################\n",
    "    standard_deviation = np.std(spectrum) # standard deviation\n",
    "    features.append(standard_deviation)\n",
    "    ###################################\n",
    "    standard_deviation_pop = statistics.stdev(spectrum) # population sample standard deviation \n",
    "    features.append(standard_deviation_pop)\n",
    "    ###################################\n",
    "    percentile15 = np.percentile(spectrum, 15)\n",
    "    features.append(percentile15)\n",
    "    ###################################\n",
    "    percentile25 = np.percentile(spectrum, 25)\n",
    "    features.append(percentile25)\n",
    "    ###################################\n",
    "    percentile50 = np.percentile(spectrum, 50)\n",
    "    features.append(percentile50)\n",
    "    ###################################\n",
    "    percentile75 = np.percentile(spectrum, 75)\n",
    "    features.append(percentile75)\n",
    "    ###################################\n",
    "    amplitude = maximum - minimum\n",
    "    features.append(amplitude)\n",
    "    ###################################\n",
    "    # mode = statistics.mode(spectrum)\n",
    "    ###################################\n",
    "    variance = statistics.variance(spectrum)\n",
    "    features.append(variance)\n",
    "    ###################################\n",
    "    interquartile_range = np.percentile(spectrum, 75) - np.percentile(spectrum, 25)\n",
    "    features.append(interquartile_range)\n",
    "    ###################################\n",
    "    semi_interquartile_range = (np.percentile(spectrum, 75) - np.percentile(spectrum, 25))/2 \n",
    "    features.append(semi_interquartile_range)\n",
    "    ###################################\n",
    "    coefficient_of_variation = standard_deviation/average\n",
    "    features.append(coefficient_of_variation)\n",
    "    ###################################\n",
    "    skewness = (3 * (average - median))/standard_deviation\n",
    "    features.append(skewness)   \n",
    "    ###################################\n",
    "    kurtosis = (np.percentile(spectrum, 75) - np.percentile(spectrum, 25)) / (2 * (np.percentile(spectrum, 90) - np.percentile(spectrum, 10))) \n",
    "    features.append(kurtosis)\n",
    "    ###################################\n",
    "    return features\n",
    "\n",
    "\n",
    "def accumulated_nucle_frequency_fourier(seq):\n",
    "    \n",
    "    seq = seq.upper()\n",
    "    features = []\n",
    "    spectrum = []\n",
    "    spectrumTwo = []\n",
    "    mapping = []\n",
    "    A = 0\n",
    "    C = 0\n",
    "    T = 0\n",
    "    G = 0\n",
    "    for i in range(len(seq)):\n",
    "        if seq[i] == 'A':\n",
    "            A += 1\n",
    "            mapping.append(A / (i + 1))\n",
    "        elif seq[i] == 'C':\n",
    "            C += 1\n",
    "            mapping.append(C / (i + 1))\n",
    "        elif seq[i] == 'T' or seq[i] == 'U':\n",
    "            T += 1\n",
    "            mapping.append(T / (i + 1))\n",
    "        else:\n",
    "            G += 1\n",
    "            mapping.append(G / (i + 1))\n",
    "    Fmap = fft(mapping)\n",
    "    for i in range(len(mapping)):\n",
    "        specTotal = (abs(Fmap[i])**2)\n",
    "        specTwo = (abs(Fmap[i]))\n",
    "        spectrum.append(specTotal)\n",
    "        spectrumTwo.append(specTwo)\n",
    "    \n",
    "    features = feature_extraction(spectrum, spectrumTwo)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(final_df: pd.DataFrame, columns: list):\n",
    "    features_dict = {}\n",
    "\n",
    "    if not os.path.isfile('./data/features.json'):\n",
    "        sequence_list = final_df['Sequence'].to_list()\n",
    "\n",
    "        features_list = []\n",
    "        for seq in tqdm(sequence_list, total=len(sequence_list)):\n",
    "            features_list.append(accumulated_nucle_frequency_fourier(seq))\n",
    "\n",
    "        features_list = np.array(features_list)\n",
    " \n",
    "        features_dict = {}\n",
    "        for i in tqdm(range(len(columns))):\n",
    "            features_dict[columns[i]] = list(features_list[:, i])\n",
    "\n",
    "        with open('./data/features.json', 'w') as f: \n",
    "            json.dump(features_dict, f)\n",
    "\n",
    "    else:\n",
    "        with open('./data/features.json', 'r') as f: \n",
    "            features_dict = json.load(f)\n",
    "    \n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['average', 'median', 'maximum', 'minimum', 'peak', 'none_levated_peak', 'sample_standard_deviation', 'population_standard_deviation', \\\n",
    "        'percentile15', 'percentile25', 'percentile50', 'percentile75', 'amplitude', 'variance', 'interquartile_range', 'semi_interquartile_range', \\\n",
    "        'coefficient_of_variation', 'skewness', 'kurtosis']\n",
    "\n",
    "if not os.path.isfile('./data/final.csv'):\n",
    "    aux_list = generate_sequences(data_df)\n",
    "\n",
    "    final_df = pd.DataFrame(aux_list)\n",
    "    final_df = final_df.dropna()\n",
    "    final_df.head()\n",
    "\n",
    "    features_dict = generate_features(final_df, columns)\n",
    "\n",
    "    for column in columns:\n",
    "        final_df[column] = features_dict[column]\n",
    "\n",
    "    final_df = final_df.dropna()\n",
    "    final_df.head()\n",
    "\n",
    "    final_df.to_csv('./data/final.csv')\n",
    "\n",
    "else:\n",
    "    final_df = pd.read_csv('./data/final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[[column for column in final_df.columns if column != 'Unnamed: 0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.groupby('Class')['Chr'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = final_df['Class'].to_list()\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(class_list)\n",
    "\n",
    "columns = ['average', 'median', 'maximum', 'minimum', 'peak', 'none_levated_peak', 'sample_standard_deviation', 'population_standard_deviation', \\\n",
    "            'percentile15', 'percentile25', 'percentile50', 'percentile75', 'amplitude', 'variance', 'interquartile_range', 'semi_interquartile_range', \\\n",
    "            'coefficient_of_variation', 'skewness', 'kurtosis']\n",
    "\n",
    "x = final_df[columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_resample, y_resample = NearMiss(sampling_strategy={0:15000, 2:15000, 3:15000, 5:15000}).fit_resample(x, y)\n",
    "x_resample, y_resample = SMOTE(sampling_strategy={1: 15000, 4:15000}, random_state=42).fit_resample(x_resample, y_resample)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_resample, y_resample, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = make_pipeline(StandardScaler(), RandomForestClassifier())\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['Helitron', 'LINE', 'LTR', 'MITE', 'SINE', 'TIR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/classifier.pkl', 'wb') as f: dill.dump(classifier, f) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
